{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs with Langchain\n",
    "\n",
    "In this recipe, we show how to obtain a model client for different providers using Langchain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies\n",
    "\n",
    "> **NOTE**: When running this recipe in [Colab](https://colab.research.google.com/), you may see an error about dependency conflicts with `google-colab 1.0.0`. You can safely ignore this error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/ibm-granite-community/utils.git \\\n",
    "    'langchain_replicate @ git+https://github.com/ibm-granite-community/langchain-replicate.git' \\\n",
    "    langchain_ollama \\\n",
    "    langchain_ibm \\\n",
    "    langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicate\n",
    "\n",
    "For instructions on how to set up Replicate, see [Getting Started with Replicate](../Getting_Started/Getting_Started_with_Replicate.ipynb).\n",
    "\n",
    "To select a Granite model, go to the [`ibm-granite`](https://replicate.com/ibm-granite) org on Replicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_replicate import ChatReplicate\n",
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "\n",
    "model = ChatReplicate(\n",
    "    model=\"ibm-granite/granite-4.0-h-small\",\n",
    "    replicate_api_token=get_env_var('REPLICATE_API_TOKEN'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama\n",
    "\n",
    "To use Ollama in Granite Cookbook recipes, you will need to run the Ollama server locally to where Jupyter notebook is running, either on your own system or in Colab. \n",
    "\n",
    "For instructions on how to set up and run Ollama, see [Getting Started with Ollama](../Getting_Started/Getting_Started_with_Ollama.ipynb).\n",
    "\n",
    "To select a Granite language model, go to the [Ollama Granite 4 models](https://ollama.com/ibm/granite4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment when using Ollama models\n",
    "# from langchain_ollama import ChatOllama\n",
    "\n",
    "# model = ChatOllama(model=\"ibm/granite4:micro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WatsonX\n",
    "\n",
    "For instructions on how to set up WatsonX, see [Getting Started with WatsonX](../Getting_Started/Getting_Started_with_WatsonX.ipynb).\n",
    "\n",
    "To select a Granite model, go to the [list of Foundation Models](https://www.ibm.com/products/watsonx-ai/foundation-models) on WatsonX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment when using ChatWatsonx models\n",
    "# from langchain_ibm import ChatWatsonx\n",
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "\n",
    "# model = ChatWatsonx(\n",
    "#     model_id=\"ibm/granite-4-h-small\",\n",
    "#     url= get_env_var(\"WATSONX_URL\"),\n",
    "#     apikey=get_env_var(\"WATSONX_APIKEY\"),\n",
    "#     project_id=get_env_var(\"WATSONX_PROJECT_ID\"),\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LM Studio\n",
    "\n",
    "For instructions on how to set up [LM Studio](https://lmstudio.ai/), see Getting Started with LM Studio.\n",
    "\n",
    "To select a Granite model, see the section Loading Models in the Getting Started notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment when using ChatOpenAI models\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# model = ChatOpenAI(\n",
    "#     base_url=\"http://localhost:1234/v1/\", # using local inference server\n",
    "#     model=\"ibm/granite-4-micro\",\n",
    "#     api_key=\"lm-studio\" # dummy API key. Not needed to run LM Studio model\n",
    "# )\n",
    "# Uncomment when using Replicate models\n",
    "from langchain_replicate import ChatReplicate\n",
    "from ibm_granite_community.notebook_utils import get_env_var    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
